{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWdDcY8dFRa7"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFc_Ul9TGg1k"
   },
   "outputs": [],
   "source": [
    "twenty_train.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1535463998340,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "wukJrudhHp22",
    "outputId": "427397ae-8ec5-4e28-abe1-3be5bb572f85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categories in the dataset\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1535463999792,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "231tGnLrG6r-",
    "outputId": "6af19bb7-7f57-48ae-a881-fbb27818f5ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\",\n",
       " 'Subject: WHAT car is this!?',\n",
       " 'Nntp-Posting-Host: rac3.wam.umd.edu',\n",
       " 'Organization: University of Maryland, College Park',\n",
       " 'Lines: 15',\n",
       " '',\n",
       " ' I was wondering if anyone out there could enlighten me on this car I saw',\n",
       " 'the other day. It was a 2-door sports car, looked to be from the late 60s/',\n",
       " 'early 70s. It was called a Bricklin. The doors were really small. In addition,',\n",
       " 'the front bumper was separate from the rest of the body. This is ',\n",
       " 'all I know. If anyone can tellme a model name, engine specs, years',\n",
       " 'of production, where this car is made, history, or whatever info you',\n",
       " 'have on this funky looking car, please e-mail.',\n",
       " '',\n",
       " 'Thanks,',\n",
       " '- IL',\n",
       " '   ---- brought to you by your neighborhood Lerxst ----',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each observation in dataset\n",
    "twenty_train.data[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1535464001054,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "PmHysGJ-LBO3",
    "outputId": "b8d08960-4bc3-40e4-aa7e-68342669753d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length 11314\n",
      "y_train length 11314\n"
     ]
    }
   ],
   "source": [
    "print('X_train length {}'.format(len(twenty_train.data)))\n",
    "print('y_train length {}'.format(len(twenty_train.target)))\n",
    "\n",
    "# so there are total 11314 datapoints in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4942,
     "status": "ok",
     "timestamp": 1535464006075,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "6VbQWDXtHJc_",
    "outputId": "ce761d83-53e3-41cf-f72b-b4952fe8cd15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In countvectorizer, we create a vector for each datapoint here, each sentence. and then against each word, we have its count.\n",
    "\n",
    "\n",
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "X_train_counts = count_vec.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "\n",
    "# this means that we have total 11314 vectors , each with length of 130107. Each word is represented in numeric terms within each datapoint. \n",
    "# for eg: first all of the sentences from the dataset are taken and are divided into different words. Each word is then assigned a unique id. such as {'word': id}\n",
    "# then a vector is created against each id. the presence of each id corresponding value i.e. if its occuring once, it will be 1, and 2 and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12311,
     "status": "ok",
     "timestamp": 1535464018492,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "4bAksOBfJU2V",
    "outputId": "3f611e5b-be0b-42f1-ccff-cc414e97d930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1535464019750,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "2CmDCoBRJifS",
    "outputId": "e6718d14-10b0-4bfc-bf4f-7aa7f2bdc4d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tg-idf \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()   \n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)      # it is important to pass count_vectors to this\n",
    "X_train_tfidf.shape\n",
    "\n",
    "# shape of this sparse matrix will be same as X_train_counts, only that it will be normalzed and will be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4380,
     "status": "ok",
     "timestamp": 1535464024198,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "2IyAk7s4YVzi",
    "outputId": "9bf26049-415a-4bfc-84e3-d39d4b303aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwRE2JFOYV1e"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf,twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4095,
     "status": "ok",
     "timestamp": 1535464029698,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "M4pgDzlGYV5K",
    "outputId": "b86eae85-18e4-4233-c83b-3d452377181c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the testing data and check the model performance\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',shuffle=True)\n",
    "X_test_count = count_vec.transform(twenty_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_count)\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# check model performance on testing data\n",
    "\n",
    "np.mean(predictions==twenty_test.target)\n",
    "\n",
    "#need to use vectorizer.transform for the test dataset, since the training dataset fixes the vocabulary (you cannot know the full vocabulary including the training set afterall). \n",
    "# Just to be clear, thats vectorizer.transform instead of vectorizer.fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1535464031082,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "2efjb7a6b-iw",
    "outputId": "c2f847b4-f796-4c72-ef24-ea420fc33386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 130107)\n",
      "(7532, 130107)\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "print(X_test_count.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(len(twenty_test.data))\n",
    "\n",
    "# So there are 7532 text_lines/ datapoints in testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1293,
     "status": "ok",
     "timestamp": 1535468734157,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "tXKL-vS0EQKr",
    "outputId": "13be464d-19ea-4887-cc20-6317810e9642"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.target_names)\n",
    "\n",
    "# so there are 20 different categories in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7056,
     "status": "ok",
     "timestamp": 1535464038228,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "DS6y7kv4YV3e",
    "outputId": "3000407b-a77e-465d-8012-a33dc4323f84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6711,
     "status": "ok",
     "timestamp": 1535464782321,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "Ivh33UTbV2nn",
    "outputId": "8e7bcfcf-4487-4fc9-fa72-01e5cf003815"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8238183749336165"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training using SVM classifier. Note : in sckikit-learn, linear SVM is written as SGDClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "                     # note : hinge loss is used for max-margin classifiers. # penalty : regularization L2 means Ridge # alpha : regulazrization parameter # n_iter : no.of iterations\n",
    "  \n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# performance evaluation\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1535468657963,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "DS6O70q7Dmax",
    "outputId": "a6e4c6bf-af3a-439a-ccae-19c763f51e51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  1,  0, ...,  9,  3, 15])"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3y6jpb61Vig"
   },
   "source": [
    "**We can see that our model accuracy has improved after moving from Naive_bayeś to SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186969,
     "status": "ok",
     "timestamp": 1535466991041,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "Bykxctpf1HfY",
    "outputId": "6420e747-9370-47f0-9964-a60b142e9723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.9067526957751458\n"
     ]
    }
   ],
   "source": [
    "#using grid search to optimize model : Naive Bayes\n",
    "\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameter_grid = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "                             # Note : above tuning parameters belong to CountVectorizer, TfidTransformer and MultinomialNB respectively\n",
    "                             # Note : There should be two underscores between estimator name and it's parameters in a Pipeline tfidf__use_idf\n",
    "  \n",
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameter_grid, n_jobs=-1)    # Note here we have provided the model object i.e. text_clf to the data which has already training data fit into it in\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)   # previous step. We can provide the object which doesnt have training data fit such as\n",
    "                                                             # text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "print(gs_clf.best_params_)\n",
    "print(gs_clf.best_score_)\n",
    "\n",
    "#The accuracy has now increased to 90.6% for the NB classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g17pzOrH44PK"
   },
   "outputs": [],
   "source": [
    "# using GridSearchCV for SVM classifier\n",
    "\n",
    "parameter_grid = {'vect__ngram_range' : [(1,1),(1,2)],'tfidf__use_idf' : (True,False),'clf-svm__alpha' : (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm,parameter_grid,n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf_svm.best_params_)\n",
    "print(gs_clf_svm.best_score_)\n",
    "\n",
    "# so here we are achieving 89.79% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LToUs75gAX2P"
   },
   "source": [
    "**Step 6: Useful tips and a touch of NLTK.**\n",
    "1. Removing stop words: (the, then etc) from the data. You should do this only when stop words are not useful for the underlying problem. In most of the text classification problems, this is indeed not useful. Let’s see if removing stop words increases the accuracy. Update the code for creating object of CountVectorizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144981,
     "status": "ok",
     "timestamp": 1535468220113,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "zhKU3xNx_Bgv",
    "outputId": "52312ce6-de49-46f2-9146-c0994ad95fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.9057804490012374\n"
     ]
    }
   ],
   "source": [
    "# using stopwords in Naive Bayeś\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect',CountVectorizer(stop_words = 'english')),('tfidf',TfidfTransformer()),('clf',MultinomialNB())])\n",
    "\n",
    "parameter_grid = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf,parameter_grid,n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf.best_params_)\n",
    "print(gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 173023,
     "status": "ok",
     "timestamp": 1535468931172,
     "user": {
      "displayName": "Smartprep",
      "photoUrl": "//lh3.googleusercontent.com/-AxJUbILQqDc/AAAAAAAAAAI/AAAAAAAAHMI/PQ1nq0mgUdk/s50-c-k-no/photo.jpg",
      "userId": "114643924031589572260"
     },
     "user_tz": -330
    },
    "id": "vNHUkDgGBlNT",
    "outputId": "731f0da6-3612-47a4-8d7d-af86df9257ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf-svm__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.8962347534028637\n"
     ]
    }
   ],
   "source": [
    "# using stopwords in SVM\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "text_clf_svm = Pipeline([('vect',CountVectorizer(stop_words = 'english')),('tfidf',TfidfTransformer()),('clf-svm',SGDClassifier())])\n",
    "\n",
    "parameter_grid = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm,parameter_grid,n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf_svm.best_params_)\n",
    "print(gs_clf_svm.best_score_)\n",
    "\n",
    "# accuracy is 89.62%"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP using SVM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
